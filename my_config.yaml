common:
    name: "my_config"
    task: "binary" # binary, multiclass, regression
    metrics:
        - f1
        - roc_auc
        - "recall@k;k=0.01"
        - "precision@k;k=0.01"
        - "ap;average=micro" # average_precision_score
        - "ap;average=weighted" # average_precision_score
        - "f1;average=binary"
    main_metric: roc_auc
    model_dir: "/home/gleb/models" # пути сохранения обученных моделей
    skip_cols: []
    selected_models:
        - catboost
        - lightgbm
        - xgboost
        - random_forest
        - lightautoml
        - tabnet

split_data:
    test_rate: 0.2
    validation_rate: 0.2 # если не казан valid_path
    binary_threshold: 0.5 # доля положительных классов относительно негативных
    stratified_split: true # стратифицированное разбиение

sampling: # начальное сэмплирование данных
    use_sampling: true   # Если поставить false, то сэмплирование делать не будем
    train_sample_size: 100000 # размер тренировочной выборки
    validation_sample_size: 100000 # размер валидационной выборки
    sample_seed: 42 # seed для сэмплирования
    balanced_sampling: true # только для классификации
    positive_rate: 0.5 # только для бинарной классификации

train:
    verbose: true
    n_folds: 3 # количество фолдов для кросс-валидации

columns:
    all_features_file: "/home/gleb/artifacts_preprocessing/final_feature_list.txt"
    category_features_file: "/home/gleb/artifacts_preprocessing/final_categorical_list.txt"
    target_col: "target"
    index_cols: ["epk_id"]

data_source: # данные уже предобработаны и не содержат пропусков, категориальные признаки закодированы в целые числа
    train_path: "/home/gleb/data/results/train_for_sota.parquet"
    valid_path: "/home/gleb/data/results/test_for_sota.parquet"

calibration:
    use_calibration: true # калибровка модели (только для бинарной классификации)
    calibration_type: "betacal" # betacal, isotonic

# Настройки специфичные для каждой модели
models:
    catboost:
        n_folds: 3
        use_custom_hyperparameters: true
        hyperparameters:
            iterations: 1000 # число итераций (деревьев), int, [100-10000]
            learning_rate: null # скорость обучения, float, [0.001-1.0]
            depth: null # глубина дерева, int, [2-16]
            l2_leaf_reg: null # L2-регуляризация листьев, float, [1-10]
            border_count: null # число бинов для числовых признаков, int, [32-255]
            random_seed: 42 # seed для генерации случайных чисел, int
            use_best_model: null # использовать лучшую модель на валидации, bool
            auto_class_weights: null # автоматические веса классов: None, 'Balanced', 'SqrtBalanced'
            one_hot_max_size: null # максимальный размер для one-hot, int, [2-255]
            random_strength: null # сила случайности при разбиении, float, [0-10]
            bagging_temperature: null # температура бэггинга, float, [0-10]
            subsample: null # доля сэмплируемых объектов, float, [0.1-1.0]
            early_stopping_rounds: null # ранняя остановка, int, [10-1000]
            min_data_in_leaf: null # мин. число объектов в листе, int, [1-100]
            max_leaves: null # макс. число листьев, int, [2-64]
            langevin: null # использовать стохастический градиент (Langevin), bool
            bootstrap_type: Bayesian # тип бутстрепа: Bayesian, Bernoulli, MVS, Poisson
    lightgbm:
        n_folds: 3
        use_custom_hyperparameters: false
        hyperparameters:
            boosting_type: 'gbdt' # тип бустинга: gbdt, dart, goss, rf
            num_boost_round: 1000 # число итераций (деревьев), int, [100-10000]
            metric: 'auc' # метрика, например auc, binary_logloss, multi_logloss
            n_jobs: 12 # число потоков, int
            verbosity: -1 # уровень логирования, int
            seed: 77 # seed для генерации случайных чисел, int
            max_depth: 16 # максимальная глубина дерева, int, [2-64]
            num_leaves: 63 # число листьев, int, [2-1024]
            learning_rate: 0.06 # скорость обучения, float, [0.001-1.0]
            min_child_samples: 4 # мин. число объектов в листе, int, [1-100]
            colsample_bytree: 0.3 # доля признаков для каждого дерева, float, [0.1-1.0]
            subsample: 0.85 # доля сэмплируемых объектов, float, [0.1-1.0]
            subsample_freq: 10 # частота сэмплирования, int
            max_bin: 100 # число бинов для числовых признаков, int, [32-255]
            min_child_weight: 1.0 # мин. вес в листе, float, [0-10]
            min_split_gain: 0.015 # мин. прирост для сплита, float, [0-1]
            reg_lambda: 5.6 # L2-регуляризация, float, [0-10]
            reg_alpha: 0.5 # L1-регуляризация, float, [0-10]
            early_stopping_rounds: 100 # ранняя остановка, int, [10-1000]
            silent: True # отключить вывод логов, bool
    xgboost:
        n_folds: 3
        use_custom_hyperparameters: false
        hyperparameters:
            n_estimators: 1000 # число деревьев, int, [100-10000]
            n_jobs: 16 # число потоков, int
            eval_metric: 'auc' # метрика, например auc, logloss, error
            early_stopping_rounds: 200 # ранняя остановка, int, [10-1000]
            tree_method: 'hist' # метод построения дерева: auto, exact, approx, hist, gpu_hist
            max_depth: 10 # максимальная глубина дерева, int, [2-64]
            learning_rate: 0.03 # скорость обучения, float, [0.001-1.0]
            max_bin: 100 # число бинов для числовых признаков, int, [32-255]
            subsample: 0.9 # доля сэмплируемых объектов, float, [0.1-1.0]
            colsample_bylevel: 0.5 # доля признаков на уровень, float, [0.1-1.0]
    random_forest:
        n_folds: 3
        use_custom_hyperparameters: false
        hyperparameters:
            n_jobs: -1 # число потоков, int
            random_state: 5 # seed для генерации случайных чисел, int
            n_estimators: 1000 # число деревьев, int, [100-10000]
            max_depth: 100 # максимальная глубина дерева, int, [2-100]
            max_samples: 0.5 # доля сэмплируемых объектов, float, [0.1-1.0]
            bootstrap: true # использовать бутстрэппинг, bool
            verbose: 0 # уровень логирования, int
    tabnet:
        n_folds: 1
        use_custom_hyperparameters: true
        hyperparameters:
            cat_emb_dim: 4 # размерность эмбеддингов для категориальных признаков, диапазон [1-16]
            n_steps: 4 # количество шагов в TabNet, диапазон [3-10]
            hidden_dim: 16 # размерность скрытого слоя, диапазон [8-128]
            decision_dim: 8 # размерность решающего слоя, диапазон [4-64]
            n_glu_layers: 3 # количество GLU слоев, диапазон [2-4]
            dropout: 0.1 # вероятность дропаута, диапазон [0.1-0.9]
            gamma: 1.5 # коэффициент затухания для масок внимания, диапазон [1.0-2.0]
            lambda_sparse: 0.0001 # коэффициент регуляризации разреженности, диапазон [0-0.01]
            virtual_batch_size: 128 # размер виртуального батча, диапазон [128-4096]
            momentum: 0.9 # параметр momentum для BatchNorm, диапазон [0.6-0.98]
            batch_size: 1024 # размер батча для обучения, диапазон [256-32768]
            epochs: 50 # количество эпох обучения, диапазон {20, 50, 100, 200, ...}
            learning_rate: 0.01 # скорость обучения, диапазон [0.001-0.025]
            early_stopping_patience: 5 # количество эпох без улучшения до остановки обучения
            weight_decay: 1e-5 # L2-регуляризация для оптимизатора
            scale_numerical: true # масштабировать ли числовые признаки
            scale_method: "standard" # метод масштабирования числовых признаков (standard, minmax, quantile, binning)
            n_bins: 10 # количество бинов для binning
            device: cuda # устройство для обучения (cuda/cpu)
            verbose: true # выводить ли прогресс обучения
            num_workers: 0 # количество worker-процессов для DataLoader (0 - однопроцессный режим)
            random_state: 42 # seed для генерации случайных чисел
    cemlp:
        n_folds: 1
        use_custom_hyperparameters: true
        hyperparameters:
            cat_emb_dim: 4 # размерность эмбеддингов для категориальных признаков, диапазон [2-16]
            hidden_dims: [64, 32] # список размерностей скрытых слоев, например [32, 16], значения 8-256
            activation: relu # функция активации: relu, leaky_relu, selu, elu, gelu, swish, prelu
            dropout: 0.1 # вероятность дропаута, диапазон [0.0-0.9]
            feature_dropout: 0.0 # dropout для входных признаков (feature-wise), диапазон [0.0-0.5]
            normalization: batch # тип нормализации: 'batch', 'layer', 'ghost_batch'
            virtual_batch_size: 128 # размер виртуального батча для ghost_batch нормализации, диапазон [32-1024]
            momentum: 0.9 # momentum для BatchNorm/GhostBatchNorm, диапазон [0.6-0.99]
            initialization: he_normal # метод инициализации весов: he_normal, he_uniform, xavier_normal, xavier_uniform, uniform, normal, constant, ones, zeros
            constant_value: 0.001 # значение для constant инициализации
            leaky_relu_negative_slope: 0.1 # отрицательный наклон для leaky_relu, диапазон [0.01-0.3]
            dynamic_emb_size: false # использовать динамический размер эмбеддинга: true/false
            min_emb_dim: 2 # min размер эмбеддинга при dynamic_emb_size, диапазон [1-16]
            max_emb_dim: 16 # max размер эмбеддинга при dynamic_emb_size, диапазон [2-64]
            batch_size: 1024 # размер батча, диапазон [32-4096]
            epochs: 50 # количество эпох, диапазон [10-200]
            learning_rate: 0.001 # скорость обучения, диапазон [0.0001-0.01]
            weight_decay: 1e-5 # L2-регуляризация, диапазон [0-0.001]
            early_stopping_patience: 5 # patience для early stopping, диапазон [2-20]
            lr_scheduler_patience: 10 # patience для ReduceLROnPlateau, диапазон [2-20]
            lr_scheduler_factor: 0.5 # factor для ReduceLROnPlateau, диапазон [0.1-0.9]
            scale_numerical: true # масштабировать ли числовые признаки: true/false
            scale_method: standard # метод масштабирования: standard, minmax, quantile, binning
            n_bins: 10 # количество бинов для binning, диапазон [2-100]
            device: cuda # устройство для обучения: cuda, cpu
            output_dim: 1 # размерность выхода (1 для binary/regression, n_classes для multiclass)
            verbose: true # выводить прогресс: true/false
            num_workers: 0 # количество worker-процессов, диапазон [0-8]
            random_state: 42 # seed для генерации случайных чисел, любое целое число
            use_self_attention: false # использовать self-attention: true/false
            num_attention_heads: 4 # количество голов внимания при use_self_attention, диапазон [1-16]
